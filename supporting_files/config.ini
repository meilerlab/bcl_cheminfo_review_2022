[bcl]
# specify bcl executable
#bcl: /sb/apps/bcl/bcl/build/linux64_release/bin/bcl-apps-static.exe
bcl: /home/ben/workspace/bcl/build/linux64_release/bin/bcl-apps-static.exe

[main]
# specify whether the bcl executable runs locally, uses pbs, or gnu-parallels for job execution
#gparallel: '-j4'
random_seed:

[variables]
# activity cutoff used in objective functions
cutoff: 0.5
# parity determines whether values smaller than the cutoff are considered active (0) or inactive (1)
parity: 1

# choose one training objective-function
#objective-function: 'RMSD'
#objective-function: 'MAE_NMAD'
objective-function: 'AucRocCurve(cutoff=%(cutoff)s,parity=%(parity)s,x_axis_log=1,min fpr=0.001,max fpr=0.1)'
#objective-function: 'AucRocCurve(cutoff=%(cutoff)s,parity=%(parity)s,x_axis_log=0,min fpr=0,max fpr=1)'
#objective-function: 'BinaryOperation(op=*,lhs=ContingencyMatrixMeasure(cutoff=%(cutoff)s,parity=%(parity)s,measure=TPR),rhs=ContingencyMatrixMeasure(cutoff=%(cutoff)s,parity=%(parity)s,measure=TNR))'
#objective-function: 'ContingencyMatrixMeasure(cutoff=%(cutoff)s,parity=%(parity)s,measure=MCC,adjustable cutoff=True)'
#objective-function: 'EnrichmentAverage(cutoff=%(cutoff)s,enrichment max=0.01,step size=0.00001,parity=%(parity)s)'
#objective-function: 'InformationGainRatio(cutoff=%(cutoff)s,measure='PPV',parity=%(parity)s)'

# Choose misc. training hyperparameters
visdrop: 0.05
hiddrop: 0.25
hiddenneurons: 32
dl_hiddenneurons:512
alpha: 0.5
eta: 0.05
balanceratio: 0.1
droptype: Zero
report_freq: 1

# Choose misc. training hyperparameters for autoencoder
ae_iter: 100
ae_model_path: AutoEncoderModels
ae_hiddrop: 0.25
ae_hiddenneurons: 32

[learning]

#############################
#                           #
# Non-ANN ML base settings  #
#                           #
#############################

#learning-method: 'DecisionTree( objective function=%(objective-function)s,partitioner=Gini,activity cutoff=%(cutoff)s,node score = RatingTimesInitialNumIncorrect,min split = 5)'
#learning-method: 'SupportVectorMachine( objective function=%(objective-function)s, kernel = RBF( gamma=0.4),iterations=500,cost=0.1,gap_threshold=0.1)' 
#learning-method: 'KappaNearestNeighbor( objective function=%(objective-function)s, min kappa=1, max kappa=25)'
#learning-method: 'Kohonen( objective function=%(objective-function)s, map dimensions(20.0,20.0), steps per update=0, length=20, radius=10, neighbor kernel = Gaussian,initializer=RandomlyChosenElements)'
#learning-method: 'OpenclIterateSequentialMinimalOptimization ( objective function=RMSD, gamma=0.4,iterations=500,cost=0.1 )'
#learning-method: 'OpenCLResilientPropagation(objective function=RMSD,steps per call=0, hidden architecture(8))'
#learning-method: 'OpenCLSimplePropagation(hidden architecture(8),eta=0.05,alpha=0.5,steps per call=0,objective function=RMSD)'
#learning-method: 'LinearRegression( objective function=%(objective-function)s,solver=Cholesky(smoothing=0.20))'

#############################
#                           #
#        Shallow ANNs 	    #
#                           #
#############################

#Old reliable classifier
learning-method: 'NeuralNetwork( transfer function = Sigmoid, weight update = Simple(alpha=%(alpha)s,eta=%(eta)s),dropout(%(visdrop)s,%(hiddrop)s),objective function = %(objective-function)s,scaling=AveStd,steps per update=1,hidden architecture(%(hiddenneurons)s),balance=True,balance target ratio=%(balanceratio)s,shuffle=True,input dropout type=%(droptype)s)'

#Old reliable regression with shallow AutoEncoder - note that these AutoEncoders override the architecture of the subsequent ANN, so they are more like general pre-trainers. For legitimate AutoEncoding, run with 0 iterations on the NeuralNetwork and use the output compacted model with EncodeByModel to turn it into a new input layer. Then, just train with that new bin file using the standard workflow.
#learning-method: 'NeuralNetwork(initial network=AutoEncoder(shuffle=True,balance=False,balance max repeats=100000, transfer function = Sigmoid, weight update = Simple(alpha=%(alpha)s,eta=%(eta)s),dropout(%(visdrop)s,%(ae_hiddrop)s),scaling=AveStd,steps per update=1,hidden architecture(%(ae_hiddenneurons)s),input dropout type=%(droptype)s, iterations=%(ae_iter)s,model storage path=%(ae_model_path)s),transfer function = Sigmoid, weight update = Simple(alpha=%(alpha)s,eta=%(eta)s),dropout(%(visdrop)s,%(hiddrop)s),objective function = %(objective-function)s,scaling=AveStd,steps per update=1,hidden architecture(%(hiddenneurons)s),balance=False,balance target ratio=%(balanceratio)s,shuffle=True,input dropout type=%(droptype)s)'

# Old reliable with existing AutoEncoder
#learning-method: 'NeuralNetwork(initial network=File(directory=%(ae_model_path)s,prefix=model),transfer function = Sigmoid, weight update = Simple(alpha=%(alpha)s,eta=%(eta)s),dropout(%(visdrop)s,%(hiddrop)s),objective function = %(objective-function)s,scaling=AveStd,steps per update=1,hidden architecture(%(hiddenneurons)s),balance=False,balance target ratio=%(balanceratio)s,shuffle=True,input dropout type=%(droptype)s)'

#############################
#                           #
#        Deep ANNs 	    #
#                           #
#############################

# Multitasking deep classifier
#learning-method: 'NeuralNetwork(balance=True,balance target ratio=0.1,balance max repeats=100000,transfer function = Rectifier(0.05),weight update = Simple(alpha=0.5,eta=0.001),input dropout type=Zero,objective function =%(objective-function)s,input noise=0.0,iteration weight update=Attenuate(0.000,0,0,0.01),shuffle=True,steps per update=10,dropout(0.05,0.05,0.05),hidden architecture(256,32), rescale output dynamic range=True,rmsd report frequency=%(report_freq)s,scaling=AveStd)'
#learning-method: 'NeuralNetwork(balance=True,balance target ratio=0.1,balance max repeats=100000,transfer function = Rectifier(0.05),weight update = Simple(alpha=0.5,eta=0.001),input dropout type=Zero,objective function =%(objective-function)s,input noise=0.0,iteration weight update=Attenuate(0.000,0,0,0.01),shuffle=True,steps per update=10,dropout(0.05,0.25,0.05),hidden architecture(256,32), rescale output dynamic range=True,rmsd report frequency=%(report_freq)s,scaling=AveStd)'

#Regression
#learning-method: 'NeuralNetwork(balance=False,transfer function = Rectifier(0.05),weight update = Simple(alpha=0.5,eta=0.0025),input dropout type=Zero,objective function =%(objective-function)s,input noise=0.0,iteration weight update=Attenuate(0.000,0,0,0.01),shuffle=True,steps per update=10,dropout(0.05,0.25,0.25,0.05),hidden architecture(128,32,8), rescale output dynamic range=True,rmsd report frequency=%(report_freq)s,scaling=AveStd)'

# Regression with shallow AutoEncoder - note that these AutoEncoders override the architecture of the subsequent DNN, so they are more like general pre-trainers. For legitimate AutoEncoding, run with 0 iterations on the NeuralNetwork and use the output compacted model with EncodeByModel to turn it into a new input layer. Then, just train with that new bin file using the standard workflow.
#learning-method: 'NeuralNetwork(initial network=AutoEncoder(shuffle=True,balance=False,balance max repeats=100000, transfer function = Sigmoid, weight update = Simple(alpha=%(alpha)s,eta=%(eta)s),dropout(%(visdrop)s,%(ae_hiddrop)s),scaling=AveStd,steps per update=1,hidden architecture(%(ae_hiddenneurons)s),input dropout type=%(droptype)s, iterations=%(ae_iter)s,model storage path=%(ae_model_path)s),balance=False,balance target ratio=0.1,balance max repeats=100000,transfer function = Rectifier(0.05),weight update = Simple(alpha=0.5,eta=0.001),input dropout type=Zero,objective function =%(objective-function)s,input noise=0.0,iteration weight update=Attenuate(0.000,0,0,0.01),shuffle=True,steps per update=10,dropout(0.05,0.25,0.05),hidden architecture(512,32), rescale output dynamic range=True,rmsd report frequency=%(report_freq)s,scaling=AveStd)'

# Regression with existing AutoEncoder
#learning-method: 'NeuralNetwork(initial network=File(directory=%(ae_model_path)s,prefix=model),balance=False,balance target ratio=0.1,balance max repeats=100000,transfer function = Rectifier(0.05),weight update = Simple(alpha=0.5,eta=0.001),input dropout type=Zero,objective function =%(objective-function)s,input noise=0.0,iteration weight update=Attenuate(0.000,0,0,0.01),shuffle=True,steps per update=10,dropout(0.05,0.25,0.05),hidden architecture(512,32), rescale output dynamic range=True,rmsd report frequency=%(report_freq)s,scaling=AveStd)'

#############################
#                           #
#        Validation 	    #
#                           #
#############################

# maximum training iterations of chosen learning-method
max-iterations: 250
monitor-independent-set:
max-minutes: 24000
result_averaging_window: 0

# choose one final-objective-function that is applied on the finalized model
final-objective-function: '%(objective-function)s'

[score]
# choose one dataset scoring type 
scoring-type: InformationGain

# specify one output filename to store the dataset scoring information
output_score_file: score.infogain.out

[cv]
monitoring-id-range: [0,4]
independent-id-range: [0,4]
cross-validations: 5
cv-repeats: 1 
override-memory-multiplier: 2.50

# where to store the model, options = Db, File, or None
store-model: File
id: ChangeMe
print-independent-predictions:
show-status:

[descriptor-selection-model-dependent]
memory-offset: 200
